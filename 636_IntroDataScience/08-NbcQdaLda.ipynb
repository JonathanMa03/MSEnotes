{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes, QDA, and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes Classifier\n",
    "- In general, we can use Bayes' rule (and the law of total probability) to infer discrete classes $C_k$ for a given $\\boldsymbol{x}$ set of features\n",
    "    - $\\displaystyle P(C_k \\lvert\\,\\boldsymbol{x}) = \\frac{\\pi(C_k)\\,{\\cal{}L}_{\\!\\boldsymbol{x}}(C_k)}{Z} $ \n",
    "- Naively assuming the features are independent: $\\displaystyle {\\cal{}L}_{\\!\\boldsymbol{x}}(C_k) = \\prod_{\\alpha}^d p(x_{\\alpha} \\lvert C_k)$ \n",
    "\n",
    "### NB: Learning\n",
    "- Say for Gaussian likelihoods, we simply estimate the sample mean and variance of all features for each class $k$\n",
    "    - $\\displaystyle p(x_{\\alpha} \\lvert C_k) = G(x_{\\alpha};\\mu_{k,\\alpha}, \\sigma^2_{k,\\alpha})$\n",
    "- We have to also pick some prior for the classes using uniform or based on frequency of points in the training set\n",
    "\n",
    "### NB: Estimation\n",
    "- - Look for maximum of the posterior: $\\displaystyle \\hat{k} =  \\mathrm{arg}\\max_k \\left[ \\pi_k \\prod_{\\alpha}^d G(x_{\\alpha};\\mu_{k,\\alpha}, \\sigma^2_{k,\\alpha})\\right]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [5.006 3.428 1.462 0.246] [0.12424898 0.1436898  0.03015918 0.01110612]\n",
      "1 [5.936 2.77  4.26  1.326] [0.26643265 0.09846939 0.22081633 0.03910612]\n",
      "2 [6.588 2.974 5.552 2.026] [0.40434286 0.10400408 0.30458776 0.07543265]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# calculate feature means and variances for each class\n",
    "param = dict()  # save parameters here\n",
    "data = iris.data.copy()\n",
    "label = iris.target\n",
    "\n",
    "# Define the classes\n",
    "classes = np.unique(label)\n",
    "\n",
    "for k in classes:\n",
    "    members = (label == k)        # boolean mask\n",
    "    num = members.sum()           # count\n",
    "    prior = num / label.size      # class prior\n",
    "    \n",
    "    X = data[members, :].copy()   # slice members safely\n",
    "    mu = X.mean(axis=0)           # class mean\n",
    "    \n",
    "    X -= mu                       # mean-center\n",
    "    var = np.square(X).sum(axis=0) / (X.shape[0] - 1)  # unbiased variance\n",
    "    \n",
    "    param[k] = (prior, mu, var)   # save results\n",
    "    print(k, mu, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "# init predicted values\n",
    "k_pred = -1 * np.ones(iris.target.size)\n",
    "\n",
    "# evaluate posterior for each point and find maximum\n",
    "for i in range(iris.target.size):\n",
    "    pmax, kmax = -1, None   # initialize to nonsense values\n",
    "    for k in param:\n",
    "        prior, mu, var = param[k]\n",
    "        diff = iris.data[i,:] - mu\n",
    "        d2 = np.square(diff) / (2*var) \n",
    "        p = prior * np.exp(-d2.sum()) / np.sqrt(np.prod(2 * np.pi * var))\n",
    "        if p > pmax:\n",
    "            pmax = p\n",
    "            kmax = k\n",
    "    k_pred[i] = kmax\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=k_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.00000000e+00 2.98130936e-18 2.15237312e-25]\n",
      "1 [1.00000000e+00 3.16931184e-17 6.93802994e-25]\n",
      "2 [1.00000000e+00 2.36711261e-18 7.24095643e-26]\n",
      "3 [1.00000000e+00 3.06960607e-17 8.69063581e-25]\n",
      "4 [1.00000000e+00 1.01733735e-18 8.88579362e-26]\n",
      "5 [1.00000000e+00 2.71773169e-14 4.34428540e-21]\n",
      "6 [1.00000000e+00 2.32163910e-17 7.98827129e-25]\n",
      "7 [1.00000000e+00 1.39075122e-17 8.16699477e-25]\n",
      "8 [1.00000000e+00 1.99015585e-17 3.60646902e-25]\n",
      "9 [1.00000000e+00 7.37893147e-18 3.61549223e-25]\n",
      "10 [1.00000000e+00 9.39608901e-18 1.47462333e-24]\n",
      "11 [1.00000000e+00 3.46196432e-17 2.09362749e-24]\n",
      "12 [1.00000000e+00 2.80452047e-18 1.01019202e-25]\n",
      "13 [1.00000000e+00 1.79903266e-19 6.06057778e-27]\n",
      "14 [1.00000000e+00 5.53387950e-19 2.48503292e-25]\n",
      "15 [1.00000000e+00 6.27386346e-17 4.50986372e-23]\n",
      "16 [1.00000000e+00 1.10665843e-16 1.28241922e-23]\n",
      "17 [1.00000000e+00 4.84177304e-17 2.35001131e-24]\n",
      "18 [1.00000000e+00 1.12617475e-14 2.56717986e-21]\n",
      "19 [1.00000000e+00 1.80851332e-17 1.96392412e-24]\n",
      "20 [1.00000000e+00 2.17838161e-15 2.01398886e-22]\n",
      "21 [1.00000000e+00 1.21005713e-15 7.78859194e-23]\n",
      "22 [1.00000000e+00 4.53521976e-20 3.13007420e-27]\n",
      "23 [1.0000000e+00 3.1473268e-11 8.1753051e-19]\n",
      "24 [1.00000000e+00 1.83850701e-14 1.55375650e-21]\n",
      "25 [1.00000000e+00 6.87398999e-16 1.83037382e-23]\n",
      "26 [1.00000000e+00 3.19259795e-14 1.04514632e-21]\n",
      "27 [1.00000000e+00 1.54256198e-17 1.27439360e-24]\n",
      "28 [1.00000000e+00 8.83328510e-18 5.36807676e-25]\n",
      "29 [1.00000000e+00 9.55793483e-17 3.65257147e-24]\n",
      "30 [1.00000000e+00 2.16683674e-16 6.73053550e-24]\n",
      "31 [1.00000000e+00 3.94050016e-14 1.54667785e-21]\n",
      "32 [1.00000000e+00 1.60909243e-20 1.01327810e-26]\n",
      "33 [1.00000000e+00 7.22221654e-20 4.26185288e-26]\n",
      "34 [1.00000000e+00 6.28934800e-17 1.83169443e-24]\n",
      "35 [1.00000000e+00 2.85092612e-18 8.87400243e-26]\n",
      "36 [1.00000000e+00 7.74627940e-18 7.23562769e-25]\n",
      "37 [1.00000000e+00 8.62393373e-20 1.22363297e-26]\n",
      "38 [1.00000000e+00 4.61293612e-18 9.65544972e-26]\n",
      "39 [1.00000000e+00 2.00932459e-17 1.23775461e-24]\n",
      "40 [1.00000000e+00 1.30063407e-17 5.65768876e-25]\n",
      "41 [1.00000000e+00 1.57761736e-15 5.71721937e-24]\n",
      "42 [1.00000000e+00 1.49491068e-18 4.80033296e-26]\n",
      "43 [1.00000000e+00 1.07647471e-10 3.72134391e-18]\n",
      "44 [1.00000000e+00 1.35756868e-12 1.70832571e-19]\n",
      "45 [1.00000000e+00 3.88211276e-16 5.58781372e-24]\n",
      "46 [1.00000000e+00 5.08673490e-18 8.96015552e-25]\n",
      "47 [1.00000000e+00 5.01279330e-18 1.63656609e-25]\n",
      "48 [1.00000000e+00 5.71724524e-18 8.23133680e-25]\n",
      "49 [1.00000000e+00 7.71345599e-18 3.34999697e-25]\n",
      "50 [4.89304818e-107 8.01865280e-001 1.98134720e-001]\n",
      "51 [7.92054987e-100 9.42928316e-001 5.70716844e-002]\n",
      "52 [5.49436872e-121 4.60625359e-001 5.39374641e-001]\n",
      "53 [1.12943532e-69 9.99962100e-01 3.78996409e-05]\n",
      "54 [1.47332854e-105 9.50340836e-001 4.96591637e-002]\n",
      "55 [1.93118359e-89 9.99001346e-01 9.98653766e-04]\n",
      "56 [4.53909919e-113 6.59251466e-001 3.40748534e-001]\n",
      "57 [2.54975318e-34 9.99999688e-01 3.11951694e-07]\n",
      "58 [6.56281369e-97 9.89538471e-01 1.04615294e-02]\n",
      "59 [5.00020964e-69 9.99892836e-01 1.07163774e-04]\n",
      "60 [7.35454753e-41 9.99999686e-01 3.14391510e-07]\n",
      "61 [4.79913442e-86 9.95856383e-01 4.14361747e-03]\n",
      "62 [4.63128676e-60 9.99992459e-01 7.54127435e-06]\n",
      "63 [1.05225162e-103 9.85086756e-001 1.49132441e-002]\n",
      "64 [4.78979864e-55 9.99970006e-01 2.99939265e-05]\n",
      "65 [1.51470553e-92 9.78758747e-01 2.12412535e-02]\n",
      "66 [1.33834828e-97 9.89931070e-01 1.00689305e-02]\n",
      "67 [2.02611528e-62 9.99979927e-01 2.00731362e-05]\n",
      "68 [6.54747346e-101 9.94199573e-001 5.80042677e-003]\n",
      "69 [3.01627557e-58 9.99991260e-01 8.73995881e-06]\n",
      "70 [1.05334130e-127 1.60936052e-001 8.39063948e-001]\n",
      "71 [1.24820154e-70 9.99774330e-01 2.25669798e-04]\n",
      "72 [3.29475275e-119 9.24581240e-001 7.54187597e-002]\n",
      "73 [1.31417508e-95 9.97939767e-01 2.06023272e-03]\n",
      "74 [3.00311724e-83 9.98273563e-01 1.72643719e-03]\n",
      "75 [2.53674658e-92 9.86537189e-01 1.34628107e-02]\n",
      "76 [1.55890917e-111 9.10226024e-001 8.97739760e-002]\n",
      "77 [7.01428235e-136 7.98960712e-002 9.20103929e-001]\n",
      "78 [5.03452839e-99 9.85495670e-01 1.45043304e-02]\n",
      "79 [1.43905228e-41 9.99998398e-01 1.60157383e-06]\n",
      "80 [1.25156712e-54 9.99995500e-01 4.50013931e-06]\n",
      "81 [8.76953858e-48 9.99998257e-01 1.74256024e-06]\n",
      "82 [3.44718104e-62 9.99966380e-01 3.36198712e-05]\n",
      "83 [1.08730157e-132 6.13435477e-001 3.86564523e-001]\n",
      "84 [4.11985160e-97 9.91829740e-01 8.17025974e-03]\n",
      "85 [1.14083488e-102 8.73410672e-001 1.26589328e-001]\n",
      "86 [2.24733901e-110 7.97179533e-001 2.02820467e-001]\n",
      "87 [4.87062951e-88 9.99297792e-01 7.02208393e-04]\n",
      "88 [2.02867234e-72 9.99762010e-01 2.37989782e-04]\n",
      "89 [2.22789974e-69 9.99946095e-01 5.39051420e-05]\n",
      "90 [5.11070876e-81 9.99851018e-01 1.48981946e-04]\n",
      "91 [5.77484105e-99 9.88539944e-01 1.14600562e-02]\n",
      "92 [5.14673589e-66 9.99959105e-01 4.08954045e-05]\n",
      "93 [1.33281585e-34 9.99999728e-01 2.71626355e-07]\n",
      "94 [6.09414397e-77 9.99803367e-01 1.96633066e-04]\n",
      "95 [1.42427560e-72 9.99823554e-01 1.76446253e-04]\n",
      "96 [8.30264127e-77 9.99669245e-01 3.30754783e-04]\n",
      "97 [1.83551998e-82 9.98860085e-01 1.13991543e-03]\n",
      "98 [5.71035034e-30 9.99999691e-01 3.09473861e-07]\n",
      "99 [3.99645943e-73 9.99820427e-01 1.79572631e-04]\n",
      "100 [3.99375467e-249 1.03103165e-010 1.00000000e+000]\n",
      "101 [1.22865866e-149 2.72440613e-002 9.72755939e-001]\n",
      "102 [2.46066087e-216 2.32748775e-007 9.99999767e-001]\n",
      "103 [2.86483075e-173 2.29095423e-003 9.97709046e-001]\n",
      "104 [8.29988401e-214 3.17538449e-007 9.99999682e-001]\n",
      "105 [1.37118191e-267 3.80745512e-010 1.00000000e+000]\n",
      "106 [3.44408994e-107 9.71988456e-001 2.80115445e-002]\n",
      "107 [3.74192883e-224 1.78204658e-006 9.99998218e-001]\n",
      "108 [5.56464372e-188 5.82319061e-004 9.99417681e-001]\n",
      "109 [2.05244277e-260 2.46166168e-012 1.00000000e+000]\n",
      "110 [8.66940466e-159 4.89523544e-004 9.99510476e-001]\n",
      "111 [4.22020044e-163 3.16864332e-003 9.96831357e-001]\n",
      "112 [4.36005895e-190 6.23082087e-006 9.99993769e-001]\n",
      "113 [6.14225649e-151 1.42341428e-002 9.85765857e-001]\n",
      "114 [2.20142563e-186 1.39324664e-006 9.99998607e-001]\n",
      "115 [2.94994452e-191 6.12838549e-007 9.99999387e-001]\n",
      "116 [2.90907580e-168 2.15284325e-003 9.97847157e-001]\n",
      "117 [1.34760822e-281 2.87299603e-012 1.00000000e+000]\n",
      "118 [2.78640223e-306 1.15146918e-012 1.00000000e+000]\n",
      "119 [2.08250961e-123 9.56162608e-001 4.38373916e-002]\n",
      "120 [2.19416867e-217 1.71216566e-008 9.99999983e-001]\n",
      "121 [3.32579098e-145 1.51871795e-002 9.84812821e-001]\n",
      "122 [6.25135701e-269 1.17087241e-009 9.99999999e-001]\n",
      "123 [4.4151346e-135 1.3604316e-001 8.6395684e-001]\n",
      "124 [6.31571604e-201 1.30051186e-006 9.99998699e-001]\n",
      "125 [5.25734682e-203 9.50798871e-006 9.99990492e-001]\n",
      "126 [1.47639092e-129 2.06770316e-001 7.93229684e-001]\n",
      "127 [8.77284065e-134 1.13058943e-001 8.86941057e-001]\n",
      "128 [5.23080010e-194 1.39571937e-005 9.99986043e-001]\n",
      "129 [7.01489211e-179 8.23251828e-004 9.99176748e-001]\n",
      "130 [6.30681975e-218 1.21449677e-006 9.99998786e-001]\n",
      "131 [2.53902018e-247 4.66889091e-010 1.00000000e+000]\n",
      "132 [2.21081169e-201 2.00031588e-006 9.99998000e-001]\n",
      "133 [1.12861322e-128 7.11894831e-001 2.88105169e-001]\n",
      "134 [8.11486921e-151 4.90099218e-001 5.09900782e-001]\n",
      "135 [7.41906776e-249 1.44805003e-010 1.00000000e+000]\n",
      "136 [1.00450344e-215 9.74335656e-009 9.99999990e-001]\n",
      "137 [1.34671583e-167 2.18698889e-003 9.97813011e-001]\n",
      "138 [1.99471568e-128 1.99989428e-001 8.00010572e-001]\n",
      "139 [8.44046556e-185 6.76912636e-006 9.99993231e-001]\n",
      "140 [2.33436482e-218 7.45622014e-009 9.99999993e-001]\n",
      "141 [2.17913862e-183 6.35266288e-007 9.99999365e-001]\n",
      "142 [1.22865866e-149 2.72440613e-002 9.72755939e-001]\n",
      "143 [3.42681362e-229 6.59701547e-009 9.99999993e-001]\n",
      "144 [2.01157448e-232 2.62063564e-010 1.00000000e+000]\n",
      "145 [1.07851850e-187 7.91554279e-007 9.99999208e-001]\n",
      "146 [1.06139156e-146 2.77057506e-002 9.72294249e-001]\n",
      "147 [1.84690030e-164 4.39840242e-004 9.99560160e-001]\n",
      "148 [1.43999573e-195 3.38415579e-007 9.99999662e-001]\n",
      "149 [2.77147961e-143 5.98790302e-002 9.40120970e-001]\n",
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "# init predicted values\n",
    "k_pred = -1 * np.ones(iris.target.size)\n",
    "\n",
    "# evaluate posterior for each point and find maximum\n",
    "for i in range(iris.target.size):\n",
    "    pmax, kmax = -1, None   # initialize to nonsense values\n",
    "    parr = np.zeros_like(classes, dtype=np.float64) # class probabilities\n",
    "    for k in classes:\n",
    "        prior, mu, var = param[k]\n",
    "        diff = iris.data[i,:] - mu\n",
    "        d2 = np.square(diff) / (2*var) \n",
    "        p = prior * np.exp(-d2.sum()) / np.sqrt(np.prod(2 * np.pi * var))\n",
    "        parr[k] = p # save\n",
    "        if p > pmax:\n",
    "            pmax = p\n",
    "            kmax = k\n",
    "    print (i, parr / parr.sum()) # normalize\n",
    "    k_pred[i] = kmax\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=k_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "# run sklearn's version - read up on differences if interested\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+000, 1.35784265e-018, 7.11283512e-026],\n",
       "       [1.00000000e+000, 1.51480769e-017, 2.34820051e-025],\n",
       "       [1.00000000e+000, 1.07304179e-018, 2.34026774e-026],\n",
       "       [1.00000000e+000, 1.46619543e-017, 2.95492722e-025],\n",
       "       [1.00000000e+000, 4.53291917e-019, 2.88389975e-026],\n",
       "       [1.00000000e+000, 1.49094245e-014, 1.75752068e-021],\n",
       "       [1.00000000e+000, 1.10262691e-017, 2.71144689e-025],\n",
       "       [1.00000000e+000, 6.53644612e-018, 2.77336308e-025],\n",
       "       [1.00000000e+000, 9.42227052e-018, 1.20443161e-025],\n",
       "       [1.00000000e+000, 3.42348334e-018, 1.20750647e-025],\n",
       "       [1.00000000e+000, 4.38090482e-018, 5.06830427e-025],\n",
       "       [1.00000000e+000, 1.65766943e-017, 7.24748728e-025],\n",
       "       [1.00000000e+000, 1.27573119e-018, 3.28718898e-026],\n",
       "       [1.00000000e+000, 7.73742183e-020, 1.86207920e-027],\n",
       "       [1.00000000e+000, 2.43526387e-019, 8.23627924e-026],\n",
       "       [1.00000000e+000, 3.04074398e-017, 1.66211400e-023],\n",
       "       [1.00000000e+000, 5.42610885e-017, 4.60661360e-024],\n",
       "       [1.00000000e+000, 2.33427918e-017, 8.15420956e-025],\n",
       "       [1.00000000e+000, 6.06809097e-015, 1.02748629e-021],\n",
       "       [1.00000000e+000, 8.54558679e-018, 6.78962703e-025],\n",
       "       [1.00000000e+000, 1.13506227e-015, 7.65275253e-023],\n",
       "       [1.00000000e+000, 6.22989337e-016, 2.90267936e-023],\n",
       "       [1.00000000e+000, 1.89645477e-020, 9.48817058e-028],\n",
       "       [1.00000000e+000, 1.99398085e-011, 3.68049199e-019],\n",
       "       [1.00000000e+000, 1.00058879e-014, 6.15534153e-022],\n",
       "       [1.00000000e+000, 3.49841784e-016, 6.62285086e-024],\n",
       "       [1.00000000e+000, 1.75721873e-014, 4.10706738e-022],\n",
       "       [1.00000000e+000, 7.26529230e-018, 4.36708593e-025],\n",
       "       [1.00000000e+000, 4.11331036e-018, 1.80735687e-025],\n",
       "       [1.00000000e+000, 4.67240237e-017, 1.27884939e-024],\n",
       "       [1.00000000e+000, 1.07710163e-016, 2.38609460e-024],\n",
       "       [1.00000000e+000, 2.17820325e-014, 6.12672716e-022],\n",
       "       [1.00000000e+000, 6.58781348e-021, 3.14606961e-027],\n",
       "       [1.00000000e+000, 3.04886902e-020, 1.36260578e-026],\n",
       "       [1.00000000e+000, 3.04840351e-017, 6.32348452e-025],\n",
       "       [1.00000000e+000, 1.29727473e-018, 2.87999446e-026],\n",
       "       [1.00000000e+000, 3.59748108e-018, 2.45102343e-025],\n",
       "       [1.00000000e+000, 3.65380825e-020, 3.81384200e-027],\n",
       "       [1.00000000e+000, 2.11976920e-018, 3.13900962e-026],\n",
       "       [1.00000000e+000, 9.51488554e-018, 4.23900719e-025],\n",
       "       [1.00000000e+000, 6.10454787e-018, 1.90690879e-025],\n",
       "       [1.00000000e+000, 8.16634024e-016, 2.02011747e-024],\n",
       "       [1.00000000e+000, 6.71335191e-019, 1.53850038e-026],\n",
       "       [1.00000000e+000, 6.99329490e-011, 1.72796105e-018],\n",
       "       [1.00000000e+000, 8.06641165e-013, 7.44897611e-020],\n",
       "       [1.00000000e+000, 1.95284044e-016, 1.97347120e-024],\n",
       "       [1.00000000e+000, 2.34216163e-018, 3.04846703e-025],\n",
       "       [1.00000000e+000, 2.30742580e-018, 5.37811977e-026],\n",
       "       [1.00000000e+000, 2.63876217e-018, 2.79566024e-025],\n",
       "       [1.00000000e+000, 3.58192707e-018, 1.11709612e-025],\n",
       "       [3.21380935e-109, 8.04037666e-001, 1.95962334e-001],\n",
       "       [7.27347795e-102, 9.45169639e-001, 5.48303606e-002],\n",
       "       [1.87142859e-123, 4.56151317e-001, 5.43848683e-001],\n",
       "       [4.26269818e-071, 9.99968751e-001, 3.12488556e-005],\n",
       "       [1.03323094e-107, 9.52441811e-001, 4.75581888e-002],\n",
       "       [2.87936711e-091, 9.99119627e-001, 8.80372566e-004],\n",
       "       [2.24730551e-115, 6.58952285e-001, 3.41047715e-001],\n",
       "       [5.06775312e-035, 9.99999767e-001, 2.33206910e-007],\n",
       "       [6.89469668e-099, 9.90316309e-001, 9.68369084e-003],\n",
       "       [1.94537625e-070, 9.99909746e-001, 9.02536083e-005],\n",
       "       [1.07499306e-041, 9.99999765e-001, 2.35068227e-007],\n",
       "       [8.39601787e-088, 9.96238286e-001, 3.76171369e-003],\n",
       "       [2.74599487e-061, 9.99993984e-001, 6.01632484e-006],\n",
       "       [8.03553250e-106, 9.86090825e-001, 1.39091754e-002],\n",
       "       [3.59466071e-056, 9.99975387e-001, 2.46126406e-005],\n",
       "       [1.95488829e-094, 9.80037003e-001, 1.99629974e-002],\n",
       "       [1.36110261e-099, 9.90687273e-001, 9.31272734e-003],\n",
       "       [1.07527617e-063, 9.99983663e-001, 1.63372809e-005],\n",
       "       [5.69746845e-103, 9.94697108e-001, 5.30289217e-003],\n",
       "       [1.94753554e-059, 9.99993006e-001, 6.99364316e-006],\n",
       "       [2.59153803e-130, 1.54494085e-001, 8.45505915e-001],\n",
       "       [4.50400610e-072, 9.99807026e-001, 1.92973847e-004],\n",
       "       [1.21785680e-121, 9.27077052e-001, 7.29229479e-002],\n",
       "       [1.46662222e-097, 9.98156519e-001, 1.84348055e-003],\n",
       "       [5.99023192e-085, 9.98460816e-001, 1.53918416e-003],\n",
       "       [3.30685495e-094, 9.87471082e-001, 1.25289184e-002],\n",
       "       [8.26876145e-114, 9.12844444e-001, 8.71555561e-002],\n",
       "       [1.16988122e-138, 7.52691316e-002, 9.24730868e-001],\n",
       "       [4.79014570e-101, 9.86480268e-001, 1.35197316e-002],\n",
       "       [2.03454675e-042, 9.99998762e-001, 1.23794211e-006],\n",
       "       [9.57864668e-056, 9.99996447e-001, 3.55251831e-006],\n",
       "       [9.25826782e-049, 9.99998651e-001, 1.34923924e-006],\n",
       "       [1.84940680e-063, 9.99972348e-001, 2.76523927e-005],\n",
       "       [2.14069731e-135, 6.12159845e-001, 3.87840155e-001],\n",
       "       [4.28650560e-099, 9.92476638e-001, 7.52336225e-003],\n",
       "       [9.18770726e-105, 8.76107551e-001, 1.23892449e-001],\n",
       "       [1.26187890e-112, 7.99294752e-001, 2.00705248e-001],\n",
       "       [7.75621059e-090, 9.99385417e-001, 6.14582528e-004],\n",
       "       [6.73000847e-074, 9.99796270e-001, 2.03730114e-004],\n",
       "       [8.52591874e-071, 9.99955234e-001, 4.47664633e-005],\n",
       "       [1.13186560e-082, 9.99873680e-001, 1.26320322e-004],\n",
       "       [5.50870062e-101, 9.89371467e-001, 1.06285328e-002],\n",
       "       [2.30685845e-067, 9.99966229e-001, 3.37713204e-005],\n",
       "       [2.61419455e-035, 9.99999798e-001, 2.02487922e-007],\n",
       "       [1.63464087e-078, 9.99832329e-001, 1.67671378e-004],\n",
       "       [4.69091927e-074, 9.99849875e-001, 1.50125138e-004],\n",
       "       [2.24117241e-078, 9.99714947e-001, 2.85053350e-004],\n",
       "       [3.79879555e-084, 9.98992363e-001, 1.00763708e-003],\n",
       "       [1.39238418e-030, 9.99999769e-001, 2.31316897e-007],\n",
       "       [1.28255689e-074, 9.99847160e-001, 1.52839987e-004],\n",
       "       [3.23245181e-254, 6.35381031e-011, 1.00000000e+000],\n",
       "       [1.06947698e-152, 2.50121636e-002, 9.74987836e-001],\n",
       "       [9.29757229e-221, 1.67915381e-007, 9.99999832e-001],\n",
       "       [8.19326602e-177, 1.99462374e-003, 9.98005376e-001],\n",
       "       [3.53169727e-218, 2.30543408e-007, 9.99999769e-001],\n",
       "       [4.66035479e-273, 2.40976995e-010, 1.00000000e+000],\n",
       "       [2.23463702e-109, 9.73514345e-001, 2.64856553e-002],\n",
       "       [9.79175322e-229, 1.34018147e-006, 9.99998660e-001],\n",
       "       [7.96991225e-192, 4.92901785e-004, 9.99507098e-001],\n",
       "       [9.77310473e-266, 1.40568403e-012, 1.00000000e+000],\n",
       "       [4.89513706e-162, 4.12884116e-004, 9.99587116e-001],\n",
       "       [1.94650439e-166, 2.77742178e-003, 9.97222578e-001],\n",
       "       [5.65575523e-194, 4.80711862e-006, 9.99995193e-001],\n",
       "       [5.02341663e-154, 1.28807070e-002, 9.87119293e-001],\n",
       "       [3.39845739e-190, 1.04253685e-006, 9.99998957e-001],\n",
       "       [3.62196003e-195, 4.50951786e-007, 9.99999549e-001],\n",
       "       [1.05264323e-171, 1.87196581e-003, 9.98128034e-001],\n",
       "       [2.37149659e-287, 1.64574932e-012, 1.00000000e+000],\n",
       "       [1.53723729e-312, 6.47406861e-013, 1.00000000e+000],\n",
       "       [6.31086515e-126, 9.58135362e-001, 4.18646381e-002],\n",
       "       [7.89162997e-222, 1.17116897e-008, 9.99999988e-001],\n",
       "       [3.56137500e-148, 1.37625971e-002, 9.86237403e-001],\n",
       "       [1.99492777e-274, 7.58240410e-010, 9.99999999e-001],\n",
       "       [7.67047647e-138, 1.29986728e-001, 8.70013272e-001],\n",
       "       [4.92284385e-205, 9.71777952e-007, 9.99999028e-001],\n",
       "       [3.71633871e-207, 7.39901993e-006, 9.99992601e-001],\n",
       "       [3.33635564e-132, 1.99928220e-001, 8.00071780e-001],\n",
       "       [1.61795839e-136, 1.07483532e-001, 8.92516468e-001],\n",
       "       [5.64350404e-198, 1.09467814e-005, 9.99989053e-001],\n",
       "       [1.54090618e-182, 7.01805717e-004, 9.99298194e-001],\n",
       "       [2.21131951e-222, 9.06238441e-007, 9.99999094e-001],\n",
       "       [2.23671366e-252, 2.96730516e-010, 1.00000000e+000],\n",
       "       [1.68672118e-205, 1.50788229e-006, 9.99998492e-001],\n",
       "       [2.68382583e-131, 7.12645144e-001, 2.87354856e-001],\n",
       "       [6.80723305e-154, 4.86199285e-001, 5.13800715e-001],\n",
       "       [6.08114349e-254, 8.98578646e-011, 1.00000000e+000],\n",
       "       [3.90608086e-220, 6.58848820e-009, 9.99999993e-001],\n",
       "       [5.02789443e-171, 1.90227600e-003, 9.98097724e-001],\n",
       "       [4.75226288e-131, 1.93183856e-001, 8.06816144e-001],\n",
       "       [1.40365213e-188, 5.23126458e-006, 9.99994769e-001],\n",
       "       [8.02061979e-223, 5.01446576e-009, 9.99999995e-001],\n",
       "       [3.87252859e-187, 4.67798053e-007, 9.99999532e-001],\n",
       "       [1.06947698e-152, 2.50121636e-002, 9.74987836e-001],\n",
       "       [7.07688248e-234, 4.42556022e-009, 9.99999996e-001],\n",
       "       [3.56898830e-237, 1.64602693e-010, 1.00000000e+000],\n",
       "       [1.56556628e-191, 5.85507962e-007, 9.99999414e-001],\n",
       "       [1.06062035e-149, 2.54457623e-002, 9.74554238e-001],\n",
       "       [7.98859322e-168, 3.70166862e-004, 9.99629833e-001],\n",
       "       [1.44378562e-199, 2.46020435e-007, 9.99999754e-001],\n",
       "       [3.25988243e-146, 5.60050092e-002, 9.43994991e-001]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class probabilities\n",
    "gnb.predict_proba(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros\n",
    "- Features are automatically treated correctly relative to each other\n",
    "    - For example, measuring similar things in different units? 1m vs 1mm\n",
    "    - The estimated mean and variance puts them on a meaningful scale\n",
    "\n",
    "### Cons\n",
    "- Independence is a strong assumption (helps with computational cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.317488</td>\n",
       "      <td>6.366470</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>12.487485</td>\n",
       "      <td>0.641854</td>\n",
       "      <td>4.867534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.007333</td>\n",
       "      <td>8.969669</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>12.481270</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.174387</td>\n",
       "      <td>4.983607</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>11.995352</td>\n",
       "      <td>0.262364</td>\n",
       "      <td>4.859812</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.912023</td>\n",
       "      <td>4.709530</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>12.254863</td>\n",
       "      <td>0.641854</td>\n",
       "      <td>4.919981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.174387</td>\n",
       "      <td>5.075174</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>12.697715</td>\n",
       "      <td>0.993252</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>4.127134</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>11.951180</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>4.962845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4.007333</td>\n",
       "      <td>7.506592</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>12.506177</td>\n",
       "      <td>0.182322</td>\n",
       "      <td>4.934474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>3.806662</td>\n",
       "      <td>7.630461</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>13.517105</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>4.927254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>3.806662</td>\n",
       "      <td>7.788626</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>11.849398</td>\n",
       "      <td>0.336472</td>\n",
       "      <td>4.941642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>3.912023</td>\n",
       "      <td>5.278115</td>\n",
       "      <td>3.806662</td>\n",
       "      <td>12.886641</td>\n",
       "      <td>0.470004</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  creatinine_phosphokinase  ejection_fraction  platelets  \\\n",
       "0    4.317488                  6.366470           2.995732  12.487485   \n",
       "1    4.007333                  8.969669           3.637586  12.481270   \n",
       "2    4.174387                  4.983607           2.995732  11.995352   \n",
       "3    3.912023                  4.709530           2.995732  12.254863   \n",
       "4    4.174387                  5.075174           2.995732  12.697715   \n",
       "..        ...                       ...                ...        ...   \n",
       "294  4.127134                  4.110874           3.637586  11.951180   \n",
       "295  4.007333                  7.506592           3.637586  12.506177   \n",
       "296  3.806662                  7.630461           4.094345  13.517105   \n",
       "297  3.806662                  7.788626           3.637586  11.849398   \n",
       "298  3.912023                  5.278115           3.806662  12.886641   \n",
       "\n",
       "     serum_creatinine  serum_sodium  DEATH_EVENT  \n",
       "0            0.641854      4.867534            1  \n",
       "1            0.095310      4.912655            1  \n",
       "2            0.262364      4.859812            1  \n",
       "3            0.641854      4.919981            1  \n",
       "4            0.993252      4.753590            1  \n",
       "..                ...           ...          ...  \n",
       "294          0.095310      4.962845            0  \n",
       "295          0.182322      4.934474            0  \n",
       "296         -0.223144      4.927254            0  \n",
       "297          0.336472      4.941642            0  \n",
       "298          0.470004      4.912655            0  \n",
       "\n",
       "[299 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes:\n",
      "\t X_train -> (209, 6)\n",
      "\t y_train -> (209,)\n",
      "test shapes:\n",
      "\t X_test -> (90, 6)\n",
      "\t y_test -> (90,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "dataset = pd.read_csv(\"heart_processed_log.csv\", index_col=0)\n",
    "display(dataset)\n",
    "\n",
    "X = dataset.drop(\"DEATH_EVENT\", axis=1).values\n",
    "y = dataset[\"DEATH_EVENT\"].values\n",
    "\n",
    "# split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# print the shapes of the training and testing sets\n",
    "print('train shapes:')\n",
    "print('\\t X_train ->', X_train.shape)\n",
    "print('\\t y_train ->', y_train.shape)\n",
    "\n",
    "print('test shapes:')\n",
    "print('\\t X_test ->', X_test.shape)\n",
    "print('\\t y_test ->', y_test.shape)\n",
    "\n",
    "# Success Rates\n",
    "def print_success_rates(y_true,y_pred):\n",
    "    n_success = np.sum(y_true == y_pred)\n",
    "    n_total   = len(y_true)\n",
    "    print(\"Number of correctly labeled points: %d of %d.  Accuracy: %.2f\" \n",
    "        % (n_success, n_total, n_success/n_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA: Full Covariance Matrix\n",
    "- Estimate the full covariance matrix for the classes: $\\displaystyle {\\cal{}L}_{\\!\\boldsymbol{x}}(C_k) =  G(\\boldsymbol{x};\\mu_k, \\Sigma_k)$\n",
    "- Handles correlated features well\n",
    "- Consider binary problem with 2 classes\n",
    "    - Taking the negative logarithm of the likelihoods we compare\n",
    "    - $\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1)^T\\,\\Sigma_1^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1) + \\ln\\,\\lvert\\Sigma_1\\lvert$ vs.\n",
    "    - $\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2)^T\\,\\Sigma_2^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2) + \\ln\\,\\lvert\\Sigma_2\\lvert$\n",
    "    - f the difference is lower than a threshold, we classify it accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points: 68 of 90.  Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "def qda_predict(X_train, y_train, X_test):\n",
    "    prior_class_0 = 0.5\n",
    "    prior_class_1 = 0.5\n",
    "\n",
    "    X_class_0 = X_train[y_train == 0] \n",
    "    X_class_1 = X_train[y_train == 1] \n",
    "    \n",
    "    mu_class_0 = np.mean(X_class_0, axis=0)\n",
    "    mu_class_1 = np.mean(X_class_1, axis=0)\n",
    "\n",
    "    sigma_class_0 = np.cov(X_class_0, rowvar=False)\n",
    "    sigma_class_1 = np.cov(X_class_1, rowvar=False)\n",
    "\n",
    "    likelihood_class_0 = multivariate_normal.pdf(X_test, mean=mu_class_0, cov=sigma_class_0)\n",
    "    likelihood_class_1 = multivariate_normal.pdf(X_test, mean=mu_class_1, cov=sigma_class_1)\n",
    "\n",
    "    posterior_class_0 = likelihood_class_0 * prior_class_0 \n",
    "    posterior_class_1 = likelihood_class_1 * prior_class_1\n",
    "\n",
    "    y_pred = np.where(posterior_class_1 > posterior_class_0, 1, 0)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "y_pred_qda = qda_predict(X_train, y_train, X_test)\n",
    "print_success_rates(y_pred_qda, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA: Same Covariance Matrix\n",
    "- When $\\Sigma_1=\\Sigma_2=\\Sigma$, the quadratic terms cancel from the difference\n",
    "    - $\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_1) $ \n",
    "    - $\\displaystyle -\\ (x\\!-\\!\\mu_2)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_2) $\n",
    "- Fewer parameters to estimate during the learning process (good if we don't have enough data)\n",
    "- Think if the data exhibits linear or nonlinear fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points: 63 of 90.  Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "def lda_predict(X_train, y_train, X_test):\n",
    "    \n",
    "    prior_class_0 = 0.5\n",
    "    prior_class_1 = 0.5\n",
    "\n",
    "    X_class_0 = X_train[y_train == 0]\n",
    "    X_class_1 = X_train[y_train == 1]\n",
    "    \n",
    "    mu_class_0 = np.mean(X_class_0, axis=0)\n",
    "    mu_class_1 = np.mean(X_class_1, axis=0)\n",
    "\n",
    "    sigma_class_0 = np.cov(X_class_0, rowvar=False)\n",
    "    sigma_class_1 = np.cov(X_class_1, rowvar=False)\n",
    "    sigma_pooled = ((X_class_0.shape[0] - 1) * sigma_class_0 + (X_class_1.shape[0] - 1) * sigma_class_1) / (X_class_0.shape[0] + X_class_1.shape[0] - 2)\n",
    "\n",
    "\n",
    "    likelihood_class_0 = multivariate_normal.pdf(X_test, mean=mu_class_0, cov=sigma_pooled)\n",
    "    likelihood_class_1 = multivariate_normal.pdf(X_test, mean=mu_class_1, cov=sigma_pooled)\n",
    "\n",
    "    posterior_class_0 = likelihood_class_0 * prior_class_0\n",
    "    posterior_class_1 = likelihood_class_1 * prior_class_1\n",
    "\n",
    "    y_pred = np.where(posterior_class_1 > posterior_class_0, 1, 0)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "y_pred_lda = lda_predict(X_train, y_train, X_test)\n",
    "print_success_rates(y_pred_lda, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sci Kit Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize features (important for LDA/QDA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "y_pred_qda = qda.predict(X_test)\n",
    "print(\"QDA Accuracy:\", accuracy_score(y_test, y_pred_qda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lda = lda.predict(X_test)\n",
    "print(\"LDA Accuracy:\", accuracy_score(y_test, y_pred_lda))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
