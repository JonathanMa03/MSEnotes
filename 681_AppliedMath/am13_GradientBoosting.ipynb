{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c800c0c-6f70-4f67-83e7-86f969c336d7",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "When we want to use predictor vectors \n",
    "$x=(x_1,\\ldots,x_k)$ to fit a model $Y=f(x)$ assume our aim is \n",
    "to minimize the expected loss\n",
    "\n",
    "$$\n",
    "E[ \\ell(Y,f(x)) ]\n",
    "$$\n",
    "\n",
    "where $\\ell$ is some loss function. Using training data \n",
    "$(x^{(i)},Y^{(i)}),i=1,\\ldots,N,$ we try to minimize \n",
    "the *empirical risk*\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^N  \\ell(Y^{(i)},f(x^{(i)}))\n",
    "$$\n",
    "\n",
    "**Examples of loss functions**\n",
    "\n",
    "For example, using squared error loss we take \n",
    "$\\ell(y,f(x)) = (y-f(x))^2.$\n",
    "\n",
    "Another example is the loss function that is equivalent to the one we use in logistic regression.  Here, **assuming that the response is $\\pm 1$ valued,** we can take\n",
    "\n",
    "$$\n",
    "\\ell(y,f(x)) = -y\\log\\left( \\frac{f(x)}{1-f(x)} \\right)\n",
    "$$\n",
    "\n",
    "To understand the connection with maximum likelihood, viewing $f(x)$ as our estimate of $P[Y=1|x]$ when we maximize the log-likelihood \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N Y^{(i)} \\log f(x^{(i)}) - Y^{(i)} \\log (1-f(x^{(i)}))\n",
    "$$\n",
    "\n",
    "this is equivalent to minimizing the expression\n",
    "\n",
    "$$\n",
    "-\\frac{1}{N}\\sum_{i=1}^N \n",
    "Y^{(i)} \n",
    "\\log \\left\\{ \\frac{f(x^{(i)})}{1-f(x^{(i)})}\\right\\}\n",
    "$$\n",
    "\n",
    "Suppose we wish to predict a categorical variable $Y$ taking values in $\\{ 1,...,K\\}$ with a probability vector $f(x) = (f_1(x),\\ldots,f_K(x))$ where we want $f_j$ close to 1 if $Y=j$ and $f_i$ close to zero otherwise. A commonly used loss function is the cross-entropy\n",
    "$$\n",
    "\\ell(y,f(x)) = - \\sum_{j=1}^k I_{y=j} \\log(f_j(x))\n",
    "$$\n",
    "and when we have data $Y^{(i)}$ and predictions $(\\hat{f}^{(i)}_1,\\ldots,\\hat{f}^{(i)}_K)$  we try to minimize\n",
    "$$\n",
    "- \\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^K I_{\\{Y^{(i)}=j\\}} \\log(\\hat{f}^{(i)}_j) \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Decision/regression stumps**\n",
    "\n",
    "To illustrate how gradient boost works, we focus on the simple case in which we build a classifier/prediction function $f$ by using only the simplest possible trees, namely, trees of depth 1. Specifying one of these trees requires selection of\n",
    "\n",
    "- a variable to split on\n",
    "- a threshold (assume all predictor variables are continuous for simplicity)\n",
    "- a final prediction to make at each leaf\n",
    "\n",
    "Let ${\\cal H}$ denote the family of all possible prediction functions.\n",
    "We will start the process of learning $f$ by picking the one $f_0$ in ${\\cal H}$ that gives a minimum empirical loss.\n",
    "\n",
    "**Regularity**\n",
    "\n",
    "It is important to note that in the case of least squares, assuming all of the $x^{(i)}$ are distinct, we can always find a function $f$ that makes our empirical loss equal to zero. We simply take $f(x^{(i)}) = Y^{(i)}$ but this would definitely lead to over-fitting. Our true aim is to minimize the expected loss which refers to a hypothetical not yet observed pair $(x,Y).$ So it makes sense to restrict our possible choices of $f$ to have some degree of *regularity*. \n",
    "\n",
    "Now that we've picked $f_0$ think of how we might modify by adding another $cf_1$ where $c \\in \\mathbb{R}$ and $f_1 \\in {\\cal H}$ to make our function \n",
    "\n",
    "$$\n",
    "f = f_0 + cf_1\n",
    "$$\n",
    "\n",
    "have improved empirical risk.  Think of $c$ as being small so we are talking about making a small perturbation of $f_0.$\n",
    "Then we can write an approximation to the empirical risk for this new function as\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^N  \\ell(Y^{(i)},f_0(x^{(i)})+cf_1(x^{(i)})) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx \n",
    "\\frac{1}{N}\\sum_{i=1}^N  \\ell(Y^{(i)},f_0(x^{(i)}))+\n",
    "c\\sum_{i=1}^N  \\ell_2(Y^{(i)},f_0(x^{(i)})) f_1(x^{(i)}).\n",
    "$$\n",
    "\n",
    "Here the term \n",
    "\n",
    "$$\n",
    "\\ell_2(Y^{(i)},f_0(x^{(i)})\n",
    "$$ \n",
    "\n",
    "means we differentiate $\\ell$ with respect to the second argument and evaluate at the current \n",
    "$(Y^{(i)},f_0(x^{(i)})).$\n",
    "\n",
    "In gradient boosting, the next step is to choose the $f_1 \\in {\\cal H}$ that minimizes this approximation, and choose the constant $c$ (the step size). We can view the expression to be minimized as a dot product \n",
    "between a gradient \n",
    "$$\n",
    "\\left[\\begin{array}{c}\n",
    "\\frac{\\partial \\ell}{\\partial f}(Y^{(1)},f_0(x^{(1)}))\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial \\ell}{\\partial f}(Y^{(N)},f_0(x^{(N)}))\\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\left[\\begin{array}{c}\n",
    "f_1(x^{(1)})\\\\\n",
    "\\vdots\\\\\n",
    "f_1(x^{(N)}))\\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "and we pick our $f_1\\in {\\cal H}$ to be as close as possible to the negative of the gradient.\n",
    "\n",
    "\n",
    "Typically we take $c\\in (0,1),$ often a small value like .1 to regularize.\n",
    "\n",
    "The gradient boosting scheme is an iterative one, at each step when we have our current $f$ which is an expression of the form\n",
    "\n",
    "$$\n",
    "f = \\sum_{j=0}^m c_j f_j\n",
    "$$ \n",
    "\n",
    "with each $f_j \\in {\\cal H}$ we compute the approximation to the empirical loss when we add a new term\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^N  \\ell(Y^{(i)},f(x^{(i)})+cf_{m+1}(x^{(i)})) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx \n",
    "\\frac{1}{N}\\sum_{i=1}^N  \\ell(Y^{(i)},f(x^{(i)}))+\n",
    "c\\sum_{i=1}^N  \\ell_2(Y^{(i)},f(x^{(i)})) f_{m+1}(x^{(i)}).\n",
    "$$\n",
    "\n",
    "again we choose $f_{m+1}\\in {\\cal H}$ to minimize this expression and a new step size $c_{m+1}$ and revise $f$ to give\n",
    "\n",
    "$$\n",
    "f = \\sum_{j=0}^m c_j f_j.\n",
    "$$ \n",
    "\n",
    "There are various schemes for choosing the sequence of step sizes e.g.\n",
    "take $c_j = c_0$ for all $j,$ $c_j = 1/(j+1)$ etc..\n",
    "\n",
    "**Least Squares**\n",
    "\n",
    "In the case of least squares, the gradient becomes $-e$ where $e$ is the vector of residuals\n",
    "$$\n",
    "e= \\left[\\begin{array}{c}\n",
    "Y^{(1)}-f(x^{(1)})\\\\\n",
    "\\vdots\\\\\n",
    "Y^{(N)}-f(x^{(N)}))\\\\\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "which is straightforward to calculate.\n",
    "\n",
    "In the general loss function, the gradient expression is referred to as the generalized residual vector.\n",
    "\n",
    "In the case of decision stumps, our update step amounts to adding on a stump predictor that closely resembles $-e.$\n",
    "\n",
    "\n",
    "**More general classes of predictors/classifiers**\n",
    "\n",
    "Our ${\\cal H}$ can consist of trees of higher depth, but for successful use of *boosting trees* this depth is usual limited to be relatively small. The whole idea is to combine many *weak learners* to produce good performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f4d6ed-5461-4732-8f0e-b7723b625d6d",
   "metadata": {},
   "source": [
    "**Gradient boosting using xgboost: Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea9cd80-51c0-45c9-bc87-0f877287b7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.2-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/jonathanma/miniforge3/envs/py313/lib/python3.13/site-packages (from xgboost) (2.3.1)\n",
      "Requirement already satisfied: scipy in /Users/jonathanma/miniforge3/envs/py313/lib/python3.13/site-packages (from xgboost) (1.16.0)\n",
      "Downloading xgboost-3.1.2-py3-none-macosx_12_0_arm64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0e1563-dccc-492a-bcf3-bff1df9a8a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv(\"penguins.csv\")\n",
    "df[\"Y\"]=df.species.map({\"Adelie\":0,\"Gentoo\":1,\"Chinstrap\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e097f3c8-45de-4452-8807-8b165a4a629f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N=df.shape[0]\n",
    "p=np.random.permutation(range(N))\n",
    "Itrain=p[0:int(2*N/3)]\n",
    "Itest=p[int(2*N/3):N]\n",
    "Xtrain=df.loc[Itrain,df.columns[2:5]].values\n",
    "Xtest=df.loc[Itest,df.columns[2:5]].values\n",
    "Ytrain=df.Y[Itrain]\n",
    "Ytest=df.Y[Itest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ac25ac-91f5-44a1-8cc9-98b0240073a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1   2\n",
       "Y                \n",
       "0      51   0   2\n",
       "1       0  38   0\n",
       "2       2   0  21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_classifier = xgb.XGBClassifier()\n",
    "xgb_classifier.fit(Xtrain,Ytrain)\n",
    "Ypred=xgb_classifier.predict(Xtest)\n",
    "pd.crosstab(Ytest,Ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507aadbf-ecd7-43e4-b45f-8124584d76aa",
   "metadata": {},
   "source": [
    "**Prediction probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2417d5-4769-4531-9771-e412cfa28719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114, 3)\n",
      "[[2.85434187e-04 9.98185813e-01 1.52872584e-03]\n",
      " [9.96466041e-01 1.04290445e-03 2.49109627e-03]\n",
      " [2.85508781e-04 9.98446643e-01 1.26782293e-03]\n",
      " [1.76152075e-03 1.22830493e-03 9.97010112e-01]\n",
      " [9.97957110e-01 7.25972757e-04 1.31685042e-03]\n",
      " [2.32859189e-03 1.10589340e-03 9.96565521e-01]\n",
      " [2.85434187e-04 9.98185813e-01 1.52872584e-03]\n",
      " [9.96466041e-01 1.04290445e-03 2.49109627e-03]\n",
      " [4.34269130e-01 1.30234882e-01 4.35496032e-01]\n",
      " [3.06196464e-03 9.95557845e-01 1.38024357e-03]\n",
      " [2.85434187e-04 9.98185813e-01 1.52872584e-03]\n",
      " [2.85508781e-04 9.98446643e-01 1.26782293e-03]\n",
      " [4.57739551e-03 9.93469357e-01 1.95323327e-03]\n",
      " [9.94353771e-01 2.30302149e-03 3.34319193e-03]\n",
      " [5.05249575e-03 1.57654732e-02 9.79182005e-01]\n",
      " [1.50978491e-02 9.76226866e-01 8.67525861e-03]\n",
      " [9.98771727e-01 2.44380382e-04 9.83823207e-04]\n",
      " [5.94037818e-04 9.97444868e-01 1.96104939e-03]\n",
      " [9.98958945e-01 2.44426192e-04 7.96573469e-04]\n",
      " [9.98930156e-01 4.14722337e-04 6.55167445e-04]\n",
      " [9.96222615e-01 1.45938213e-03 2.31797504e-03]\n",
      " [4.57739551e-03 9.93469357e-01 1.95323327e-03]\n",
      " [2.85384245e-04 9.98011112e-01 1.70351006e-03]\n",
      " [9.98816848e-01 3.93286435e-04 7.89896993e-04]\n",
      " [2.85434187e-04 9.98185813e-01 1.52872584e-03]\n",
      " [9.96662676e-01 1.46002683e-03 1.87727623e-03]\n",
      " [9.98792887e-01 2.79127678e-04 9.27996181e-04]\n",
      " [9.99113739e-01 2.34276726e-04 6.52029179e-04]\n",
      " [7.31781349e-02 9.25792038e-01 1.02985767e-03]\n",
      " [9.86001730e-01 2.28359899e-03 1.17147109e-02]\n",
      " [9.99061644e-01 2.34264502e-04 7.04087957e-04]\n",
      " [1.31346390e-01 5.12450701e-03 8.63529086e-01]\n",
      " [9.98101890e-01 6.41526538e-04 1.25664717e-03]\n",
      " [9.98281240e-01 5.50191558e-04 1.16861193e-03]\n",
      " [4.81848430e-04 9.97944415e-01 1.57368416e-03]\n",
      " [9.98006761e-01 5.50040277e-04 1.44318852e-03]\n",
      " [4.44827863e-04 9.98326600e-01 1.22857932e-03]\n",
      " [1.02321745e-03 2.05330737e-03 9.96923506e-01]\n",
      " [1.28869643e-03 2.15469929e-03 9.96556640e-01]\n",
      " [2.97393679e-04 9.98381972e-01 1.32059806e-03]\n",
      " [1.77938247e-03 9.40027006e-04 9.97280598e-01]\n",
      " [1.71735883e-03 1.39773299e-03 9.96884882e-01]\n",
      " [4.44758683e-04 9.98171329e-01 1.38386688e-03]\n",
      " [9.98101890e-01 6.41526538e-04 1.25664717e-03]\n",
      " [2.97348714e-04 9.98231113e-01 1.47162215e-03]\n",
      " [4.99382615e-04 9.87426877e-01 1.20736957e-02]\n",
      " [9.88730252e-01 7.94542022e-03 3.32428468e-03]\n",
      " [9.96466041e-01 1.04290445e-03 2.49109627e-03]\n",
      " [2.85508781e-04 9.98446643e-01 1.26782293e-03]\n",
      " [2.58957385e-03 3.37853003e-03 9.94031847e-01]\n",
      " [2.85434187e-04 9.98185813e-01 1.52872584e-03]\n",
      " [9.94008660e-01 2.64928048e-03 3.34203150e-03]\n",
      " [9.94353771e-01 2.30302149e-03 3.34319193e-03]\n",
      " [7.44735706e-04 9.81249630e-01 1.80056635e-02]\n",
      " [7.75814131e-02 2.04764441e-01 7.17654169e-01]\n",
      " [2.97393679e-04 9.98381972e-01 1.32059806e-03]\n",
      " [4.75521479e-03 1.41729172e-02 9.81071889e-01]\n",
      " [9.96466041e-01 1.04290445e-03 2.49109627e-03]\n",
      " [9.98658538e-01 2.34169987e-04 1.10732869e-03]\n",
      " [4.51685637e-02 6.01373846e-03 9.48817670e-01]\n",
      " [9.98822868e-01 2.34208521e-04 9.42873419e-04]\n",
      " [1.98502606e-03 2.22135009e-03 9.95793581e-01]\n",
      " [9.93773580e-01 2.30167783e-03 3.92466318e-03]\n",
      " [9.98969793e-01 2.79177097e-04 7.51054380e-04]\n",
      " [8.82388849e-04 3.12767643e-03 9.95989978e-01]\n",
      " [9.98371899e-01 5.50241501e-04 1.07783440e-03]\n",
      " [2.32859189e-03 1.10589340e-03 9.96565521e-01]\n",
      " [5.16322616e-04 9.98099983e-01 1.38376793e-03]\n",
      " [1.11724075e-03 9.37975228e-01 6.09075204e-02]\n",
      " [2.85434187e-04 9.98185813e-01 1.52872584e-03]\n",
      " [9.98792887e-01 2.79127678e-04 9.27996181e-04]\n",
      " [2.97393679e-04 9.98381972e-01 1.32059806e-03]\n",
      " [9.99027848e-01 2.79193337e-04 6.92976115e-04]\n",
      " [4.44827863e-04 9.98326600e-01 1.22857932e-03]\n",
      " [1.77938247e-03 9.40027006e-04 9.97280598e-01]\n",
      " [9.98549044e-01 3.93181021e-04 1.05775264e-03]\n",
      " [9.98342991e-01 2.79001950e-04 1.37805811e-03]\n",
      " [9.98875201e-01 3.93309456e-04 7.31498294e-04]\n",
      " [9.96671259e-01 1.59750041e-03 1.73130829e-03]\n",
      " [4.44827863e-04 9.98326600e-01 1.22857932e-03]\n",
      " [9.85473990e-01 1.93804887e-03 1.25879105e-02]\n",
      " [2.85508781e-04 9.98446643e-01 1.26782293e-03]\n",
      " [9.98101890e-01 6.41526538e-04 1.25664717e-03]\n",
      " [9.98946249e-01 3.68482608e-04 6.85324310e-04]\n",
      " [2.85434187e-04 9.98185813e-01 1.52872584e-03]\n",
      " [3.46028217e-04 9.98117447e-01 1.53656397e-03]\n",
      " [9.92775440e-01 1.32096268e-03 5.90360165e-03]\n",
      " [9.96691823e-01 1.37686438e-03 1.93125766e-03]\n",
      " [2.97393679e-04 9.98381972e-01 1.32059806e-03]\n",
      " [9.98830497e-01 3.09138733e-04 8.60381988e-04]\n",
      " [1.39804068e-03 1.36975665e-03 9.97232258e-01]\n",
      " [9.98371899e-01 5.50241501e-04 1.07783440e-03]\n",
      " [5.20206206e-02 1.77473556e-02 9.30231988e-01]\n",
      " [4.08609165e-03 9.93959665e-01 1.95419719e-03]\n",
      " [9.96466041e-01 1.04290445e-03 2.49109627e-03]\n",
      " [2.85508781e-04 9.98446643e-01 1.26782293e-03]\n",
      " [5.63948369e-03 9.09809396e-03 9.85262394e-01]\n",
      " [1.58072473e-03 1.88822800e-03 9.96531069e-01]\n",
      " [9.98959899e-01 3.09178780e-04 7.30908650e-04]\n",
      " [9.98549044e-01 3.93181021e-04 1.05775264e-03]\n",
      " [9.94338095e-01 1.14296260e-03 4.51898761e-03]\n",
      " [9.98477757e-01 5.14468295e-04 1.00776041e-03]\n",
      " [2.85434187e-04 9.98185813e-01 1.52872584e-03]\n",
      " [3.81845143e-03 1.74939092e-02 9.78687644e-01]\n",
      " [9.86396730e-01 3.60755925e-03 9.99569986e-03]\n",
      " [5.57955308e-03 1.66298579e-02 9.77790534e-01]\n",
      " [9.99020934e-01 2.44441355e-04 7.34674744e-04]\n",
      " [2.85467308e-04 9.98301625e-01 1.41282007e-03]\n",
      " [9.97064650e-01 7.75891705e-04 2.15942925e-03]\n",
      " [3.43055325e-03 9.95189667e-01 1.37973309e-03]\n",
      " [9.98771727e-01 2.44380382e-04 9.83823207e-04]\n",
      " [9.99002397e-01 2.34250620e-04 7.63411750e-04]\n",
      " [9.98969436e-01 2.79177009e-04 7.51364278e-04]\n",
      " [9.99113739e-01 2.34276726e-04 6.52029179e-04]]\n"
     ]
    }
   ],
   "source": [
    "pprobs=xgb_classifier.predict_proba(Xtest)\n",
    "print(pprobs.shape)\n",
    "print(pprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff9996-09e2-48d0-9627-5e840da223bd",
   "metadata": {},
   "source": [
    "## ADA Boost\n",
    "\n",
    "There are various ways that a classification method can be iteratively updated to produce improved performance. On way is via *boosting*. **Ada boosting** was introduced in 1997 by Freund and Shapiro. It has been extended to the case of regression as well.\n",
    "\n",
    "The basic idea is as follows. Assume, we have training data $(x^{(i)},Y^{(i)}),i=1,\\ldots,N$ where \n",
    "\n",
    "- each $x^{(i)}$ is a $k$-vector,\n",
    "- $Y^{(i)} =\\pm 1$\n",
    "\n",
    "And we assume we have some training algorithm that, given such a data set and importantly, weights on the observations $w_i,i=1,\\ldots,N$ produces a classifier which we can denote by\n",
    "$\\hat{Y}_w(x).$\n",
    "\n",
    "The algorithm starts by giving initial weight of $w_i = 1/N$ to each observation, and computes the classifier. Then, in successive iterations, after building the classifier, the weights are modified to give more influence on the difficult to classify observations.\n",
    "\n",
    "Here is the algorithm. \n",
    "\n",
    "At step 0 start with $w_i^{(0)} = 1/N$ for $i=1,\\ldots,N$\n",
    "\n",
    "For $j=0,\\ldots,J-1$\n",
    "\n",
    "- build a classifier $\\hat{Y}_j$ on the training data using the current weights\n",
    "\n",
    "- compute the weighted error rate \n",
    "\n",
    "$$\n",
    "e_j =\\frac{ \\sum_{i=1}^N w_i^{(j)} I(\\hat{Y}_j(x^{(i)}) \\neq Y^{(i)})}{\\sum_{i=1}^N w_i^{(j)}}\n",
    "$$\n",
    "\n",
    "- define $\\alpha_j = \\log\\left(\\frac{1-e_j}{e_j}\\right)$\n",
    "\n",
    "- Update the weights by taking\n",
    "\n",
    "$$\n",
    "w_i^{(j+1)} = w_i^{(j)} \\exp \\left( \\alpha_j I(\\hat{Y}_j(x^{(i)}) \\neq Y^{(i)})\\right)\n",
    "$$\n",
    "\n",
    "Finally, after $J$ iterations we take as our classifier \n",
    "\n",
    "$$\n",
    "\\hat{Y}(x) = \\text{sign} \\left\\{ \\sum_{j=0}^{J-1} \\alpha_j \\hat{Y}_j(x^{(i)}) \\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "**Note:** Typically we have an error rate $e_j \\in (0,1)$ and $\\alpha_j$ is well-defined. If we start off with better than random guessing, then $e_j < \\frac{1}{2}$ and $\\frac{1-e_j}{e_j}>1$ and $\\alpha_j>0,$ so  the factor we multiply our weights for \n",
    "mistakes ($e^{\\alpha_j}$) exceeds 1, while the weights for correct classifications \n",
    "are unchanged in a particular iteration.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
